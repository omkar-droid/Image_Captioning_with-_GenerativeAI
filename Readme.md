# Image Captioning with BLIP

This project demonstrates the use of the BLIP (Bootstrapped Language-Image Pretraining) model from Hugging Face's `transformers` library to generate captions for images.

## Overview

The BLIP model is used to automatically generate captions for images. This repository provides a Python script to utilize the BLIP model for image captioning.

## Requirements

- Python 3.7 or higher
- `transformers` library
- `Pillow` library

## Installation

To set up the environment and install the required libraries, run the following commands:

```bash
pip install transformers
pip install Pillow
